# Pipeline
Pipeline

Источник данных ->
-> Инструменты выгрузки: AirFlow, Python, Spark, SQL .... ->
-> Хранилище: Hadoop Hdfs, Kafka, Clickhouse, Postgre, Oracle, MongoDB, ... ->
-> Инструменты выгрузки: Flink, Spark, SparkStreaming, Storm ... ->
-> Витрина: PowerBI, Metabase, OracleBI, Grafana, Tableau, ...

Функционально процесс от начала до конца:
1. Обработка входных данных
Сбор, очистку, преобразование и другие виды подготовки данных делают с помощью кода на языках Python и SQL. Иногда Python используют в связке со Scala/Java или даже C++.
Чтобы автоматизировать подготовку данных, используют Apache Airflow, NiFi, Oozie, Luigi, cron. Специальная платформа Apache Spark позволяет параллельно обрабатывать много данных, а если это нужно делать ещё и сразу, как они приходят в хранилище, — такой процесс называется «стриминг», — то для этого подойдут Spark Streaming, Apache Storm, Flink.

2. Хранение данных
Обработанные данные сохраняются в хранилище. Хранилища бывают разных видов. Сначала компания определяет ряд параметров: сколько данных нужно обрабатывать; сколько серверов, или, по-простому, машин, для этого хватит; как сервера устроены и т.д. А затем на основе этих параметров выбирается хранилище.

Вот основные виды хранилищ:

- SQL базы данных — MySQL, PostgreSQL и пр. — используют, если объёмы данных небольшие, в них есть чёткая структура и их можно представить в виде таблиц.

- NoSQL базы данных — MongoDB, Apache Cassandra, ScyllaDB, ClickHouse и пр. — используют, если в данных нет чёткой структуры и они разнородны. Такими данными могут быть веб-документы.

- Распределённые хранилища — например, HDFS, Greenplum — используют, если данных много и их нужно обрабатывать распределённо в кластере.

- Брокеры сообщений — Kafka, RabbitMQ и пр. — используют для стриминга. Например, данные по местонахождению автобусов нужно обновлять в реальном времени как только они поступают в хранилище — геопозиция транспорта постоянно меняется.

- Облачные хранилища — например, Amazon S3 — используют, если компания решила хранить и обрабатывать данные в облаке. Тогда инфраструктуру для этого предоставляет компания-провайдер.

3. Представление данных

Представляют или применяют данные по-разному. 
- Бизнес-аналитики визуализируют статистику в Power BI, Tableau, Metabase. 
- На подготовленных данных могут обучать модели ML, используя Python-библиотеку scikit-learn, Spark ML, Amazon ML.
- Чтобы контролировать весь процесс, с помощью Elasticsearch, Logstash, Prometheus или Sentry собирают логи и метрики. 
- Тогда графики технического состояния инфраструктуры отображаются в Kibana, Grafana или New Relic, а разработчикам в обычных мессенджерах рассылаются сообщения, если что-то пошло не так.

Типовые структуры:

1. Источник -> AirFlow -> Хранилище Postge -> Витрина Metabase
Подходит для небольшого обьема данных, самый простой вариант

2. Источник -> Logstash -> Хранилище ElasticSearch -> Витрина Grafana
С помощью Logstash собирают логи и метрики, а в Grafana отображают графики технического состояния инфраструктуры.

3. Источник -> AirFlow -> Хранилище Cassandra -> Витрина Tableau
Построить витрины для данных, структура которых со временем меняется. Поскольку данные разнородные, то их нужно хранить в NoSQL базе данных, такой, как Cassandra.

4. Источник -> Spark, Luigi -> Хранилище HDFS -> Spark -> Spark ML
Автоматически подготавливать данные, накопленные за разные промежутки времени, для обучения модели ML.

5. Источник -> Храникище Kafka -> SparkStreaming - Витрина Power BI
Автоматически анализировать данные в реальном времени, чтобы настроить процесс стриминга, нужны специальные инструменты — брокеры сообщений и подходящие хранилища.

